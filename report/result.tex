\chapter{Results and discussion}

\section{Parallelization using the \textit{for} directive}

\begin{figure}[ht]
  \begin{center}
         \resizebox{160mm}{!}{\includegraphics{pic/graph_gram.eps}}
  \end{center}
  \caption{Gram-Schmidt speedup using the \textit{for} directive}
  \label{fig:gram_speedup}
\end{figure} 

As usual with very small matrices there is absolutely no speedup. The first speedup is obtained with a matrix of $600\times 600$ elements but it decreases immediately below 1. $1500\times 1500$ matrices also have a bug speedup follow by a quick decrease but it stay slightly upper than 1.
Starting from $2000\times 2000$ elements we see a speedup wich stay around 1.75 to 2 for any number of thread.

%Finally, the $2000\times 2000$ matrix has the max speedup for 16 threads and slowly decreases just after but stays on top of the other speedups. This is because the ratio data/overhead is better for the same number of threads, so there is relatively less overhead.

\section{Parallelization using the \textit{task} directive}

\begin{figure}[ht]
  \begin{center}
         \resizebox{160mm}{!}{\includegraphics{pic/graph_task.eps}}
  \end{center}
  \caption{Gram-Schmidt speedup using tasks}
  \label{fig:gram_speedup_task}
\end{figure} 

The behavior of the algorithm parallelized in that way is quite surprising as the speedup increases only for a few threads and stagnates when the treshold of 8 threads is reached, no matter the size of the data.\\

This observation is only accurate for a reasonable amount of data. If this is not the case, the speedup quickly drops below 1.\\

Those results are explained by the overheads related to task creation. Indeed, the master thread create a lot of small tasks for each iteration. Such operation is very time consuming as OpenMP has to create the data environment of the task, put it in the queue and manage the scheduling.  These tasks must also be joined before continuing with the next iteration of the outermost loop. Of course, this synchronization step takes some time, time that is not used efficiently doing some useful computations.\\

This quite big amount of overhead explains why we got those bad results. Indeed, the best speedup achieved is about 2.5, which is not really good.\\

The results' stagnation is due to the fact that passed a certain number of threads, we do not benefit from having more of them because the overhead becomes too important compared to the computation time. So, an increasing speedup cannot be achieved anymore.
